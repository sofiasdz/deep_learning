{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lower_Epoch.pynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTqm8kY3ivfTjwwVq/YTOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiasdz/deep_learning/blob/lower-epoch/Lower_Epoch_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhNU96fUj_6M"
      },
      "source": [
        "Deep Leaning TP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMzN9m4Ual4B"
      },
      "source": [
        "Kaggle Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyIAtXJzefCX"
      },
      "source": [
        "!pip install segmentation-models-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N44vLGhYelHg"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "from PIL import Image,ImageFile\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import model_selection\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "import torch\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7W3E1VBessq"
      },
      "source": [
        "def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n",
        "    component = np.zeros((height, width), np.float32)\n",
        "    component = component.reshape(-1)\n",
        "    rle = np.array([int(s) for s in rle.strip().split(' ')])\n",
        "    rle = rle.reshape(-1, 2)\n",
        "    start = 0\n",
        "    for index, length in rle:\n",
        "        start = start+index\n",
        "        end = start+length\n",
        "        component[start: end] = fill_value\n",
        "        start = end\n",
        "    component = component.reshape(width, height).T\n",
        "    return component"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vi5cK5nfndE"
      },
      "source": [
        "class SIIMDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df, data_dir, transform=None, preprocessing_fun=None, channel_first=True):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform                       # for augmentations\n",
        "        self.preprocessing_fun = preprocessing_fun       # preprocessing_fun to normalize images\n",
        "        self.channel_first = channel_first               # set channels as first dimension\n",
        "        self.image_ids = df.ImageId.values\n",
        "        self.group_by = df.groupby('ImageId')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        df = self.group_by.get_group(img_id)\n",
        "        annotations = df[' EncodedPixels'].tolist()\n",
        "        \n",
        "        img_path = os.path.join(self.data_dir, img_id + \".png\")\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = np.array(img)\n",
        "\n",
        "        mask = np.zeros(shape=(1024,1024))\n",
        "        if annotations[0] != ' -1':\n",
        "            for rle in annotations:\n",
        "                mask += run_length_decode(rle)\n",
        "        mask = (mask >= 1).astype('float32')\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "        \n",
        "        # apply augmentation\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img, mask=mask)\n",
        "            img = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        if self.preprocessing_fun:\n",
        "            img = self.preprocessing_fun(img)\n",
        "        \n",
        "        # convert shape from (width, height, channel) ----> (channel, width, height) \n",
        "        if self.channel_first:\n",
        "            img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n",
        "            mask = np.transpose(mask, (2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        return {\n",
        "            'image': torch.Tensor(img),\n",
        "            'mask': torch.Tensor(mask)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKOj8krlgCyw"
      },
      "source": [
        "def train(data_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data in tqdm(data_loader):\n",
        "        inputs = data['image']\n",
        "        labels = data['mask']\n",
        "\n",
        "        inputs = inputs.to(device, dtype=torch.float)\n",
        "        labels = labels.to(device, dtype=torch.float)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return train_loss/len(data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-XskeNhgikc"
      },
      "source": [
        "def evaluate(data_loader, model, criterion):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(data_loader):\n",
        "            inputs = data['image']\n",
        "            labels = data['mask']\n",
        "\n",
        "            inputs = inputs.to(device, dtype=torch.float)\n",
        "            labels = labels.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "    return eval_loss/len(data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUS0EH1YgwSw"
      },
      "source": [
        "Now we want to generate a new evaluate function whose input is the model, a picture and a criterion\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juILRZFhhGm7"
      },
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goNOMHuihHZd"
      },
      "source": [
        "\n",
        "from fastbook import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhB4v8mIgsEJ"
      },
      "source": [
        "uploader = widgets.FileUpload()\n",
        "uploader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LASlQsNgtji"
      },
      "source": [
        "img = PILImage.create(uploader.data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il5TE695h1jG"
      },
      "source": [
        "def evaluate_2(img, model, criterion):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        #de for data in tqdm(data_loader):\n",
        "            #inputs = data['image'] \n",
        "            #labels = data['mask']\n",
        "    #en vez de tener un data del loader vamos a tener que utilizar el img\n",
        "    inputs = img['image']\n",
        "    labels = img['mask']\n",
        "\n",
        "            inputs = inputs.to(device, dtype=torch.float)\n",
        "            labels = labels.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b56Vqx3ogq5y"
      },
      "source": [
        "# Intialize some useful variables\n",
        "\n",
        "#DATA_DIR = '../input/siim-png-images/train_png'\n",
        "#data_csv = '../input/siim-acr-pneumothorax-segmentation-data/train-rle.csv'\n",
        "DATA_DIR = '/content/train_png'\n",
        "data_csv = '/content/pneumothorax/train-rle.csv'\n",
        "batch_size = 32\n",
        "\n",
        "Encoder = 'resnet34'\n",
        "Weights = 'imagenet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLJVC0p2gsJt"
      },
      "source": [
        "# Define augmentation and preprocessing-function(according to Encoder)\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
        "    A.OneOf([A.RandomGamma(gamma_limit=(90,110)),\n",
        "             A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)], p=0.5),\n",
        "    A.Resize(width=224, height=224)\n",
        "])\n",
        "\n",
        "prep_fun = smp.encoders.get_preprocessing_fn(\n",
        "    Encoder,\n",
        "    Weights\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMmaLufygxDd"
      },
      "source": [
        "# Split data into training and validation\n",
        "\n",
        "df = pd.read_csv(data_csv)\n",
        "df_train, df_val = model_selection.train_test_split(df, test_size=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWbU-cBZg3no"
      },
      "source": [
        "# Initialize Dataset\n",
        "train_dataset = SIIMDataset(df_train,\n",
        "                            DATA_DIR,\n",
        "                            transform = transform,\n",
        "                            preprocessing_fun = prep_fun)\n",
        "\n",
        "val_dataset = SIIMDataset(df_val,\n",
        "                          DATA_DIR,\n",
        "                          transform = transform,\n",
        "                          preprocessing_fun = prep_fun)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = batch_size,\n",
        "                          shuffle = True,\n",
        "                          num_workers = 8)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size = batch_size,\n",
        "                        num_workers = 4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfNm69eSg88h"
      },
      "source": [
        "# Explore DataLoader\n",
        "\n",
        "print('Training data Info:')\n",
        "dataiter = iter(train_loader)\n",
        "data = dataiter.next()\n",
        "images,labels = data['image'],data['mask']\n",
        "print(\"shape of images : {}\".format(images.shape))\n",
        "print(\"shape of labels : {}\".format(labels.shape))\n",
        "\n",
        "print('\\nValidation data Info:')\n",
        "dataiter = iter(val_loader)\n",
        "data = dataiter.next()\n",
        "images,labels = data['image'],data['mask']\n",
        "print(\"shape of images : {}\".format(images.shape))\n",
        "print(\"shape of labels : {}\".format(labels.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am3kJXb0n4cf"
      },
      "source": [
        "def denoramlize(img):\n",
        "    img = img.permute(1,2,0)            # change shape ---> (width, height, channel)\n",
        "    mean = torch.FloatTensor([0.485, 0.456, 0.406])\n",
        "    std = torch.FloatTensor([0.229, 0.224, 0.225])\n",
        "    img = img*std + mean\n",
        "    img = np.clip(img,0,1)              # convert the pixel values range(min=0, max=1)\n",
        "    return img\n",
        "\n",
        "def imshow(img, mask):\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    a = fig.add_subplot(1, 3, 1)\n",
        "    plt.imshow(denoramlize(img), cmap='bone')\n",
        "    a.set_title(\"Original x-ray image\")\n",
        "    plt.grid(False)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    a = fig.add_subplot(1, 3, 2)\n",
        "    #imgplot = plt.imshow(torch.squeeze(mask, dim=1).permute(1,2,0), cmap='binary')\n",
        "    a.set_title(\"The mask\")\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    a = fig.add_subplot(1, 3, 3)\n",
        "    plt.imshow(denoramlize(img), cmap='bone')\n",
        "    #plt.imshow(torch.squeeze(mask, dim=1).permute(1,2,0), cmap='binary', alpha=0.3)\n",
        "    a.set_title(\"Mask on the X-ray image\")\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "    plt.grid(False)\n",
        "\n",
        "\n",
        "def show_batch_image(dataloader, num_images):\n",
        "    data = next(iter(dataloader))\n",
        "    image,mask = data['image'],data['mask']\n",
        "    img_idx = torch.randint(0, dataloader.batch_size, (num_images,))\n",
        "    for i in img_idx:\n",
        "        imshow(image[i], mask[i])\n",
        "\n",
        "show_batch_image(train_loader, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOAxQMTYop0Y"
      },
      "source": [
        "def dice_loss(input, target):\n",
        "    input = torch.sigmoid(input)\n",
        "    smooth = 1.0\n",
        "    iflat = input.view(-1)\n",
        "    tflat = target.view(-1)\n",
        "    intersection = (iflat * tflat).sum()\n",
        "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if not (target.size() == input.size()):\n",
        "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
        "                             .format(target.size(), input.size()))\n",
        "        max_val = (-input).clamp(min=0)\n",
        "        loss = input - input * target + max_val + \\\n",
        "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
        "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
        "        loss = (invprobs * self.gamma).exp() * loss\n",
        "        return loss.mean()\n",
        "        \n",
        "class MixedLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=None):\n",
        "        super().__init__()\n",
        "#         self.alpha = alpha\n",
        "#         self.focal = FocalLoss(gamma)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "#         loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
        "        loss = -torch.log(dice_loss(input, target))\n",
        "\n",
        "        return loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k9J4ck70_z4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "def get_backbone(name, pretrained=True):\n",
        "\n",
        "    \"\"\" Loading backbone, defining names for skip-connections and encoder output. \"\"\"\n",
        "\n",
        "    # TODO: More backbones\n",
        "\n",
        "    # loading backbone model\n",
        "    if name == 'resnet18':\n",
        "        backbone = models.resnet18(pretrained=pretrained)\n",
        "    elif name == 'resnet34':\n",
        "        backbone = models.resnet34(pretrained=pretrained)\n",
        "    elif name == 'resnet50':\n",
        "        backbone = models.resnet50(pretrained=pretrained)\n",
        "    elif name == 'resnet101':\n",
        "        backbone = models.resnet101(pretrained=pretrained)\n",
        "    elif name == 'resnet152':\n",
        "        backbone = models.resnet152(pretrained=pretrained)\n",
        "    elif name == 'vgg16':\n",
        "        backbone = models.vgg16_bn(pretrained=pretrained).features\n",
        "    elif name == 'vgg19':\n",
        "        backbone = models.vgg19_bn(pretrained=pretrained).features\n",
        "    # elif name == 'inception_v3':\n",
        "    #     backbone = models.inception_v3(pretrained=pretrained, aux_logits=False)\n",
        "    elif name == 'densenet121':\n",
        "        backbone = models.densenet121(pretrained=True).features\n",
        "    elif name == 'densenet161':\n",
        "        backbone = models.densenet161(pretrained=True).features\n",
        "    elif name == 'densenet169':\n",
        "        backbone = models.densenet169(pretrained=True).features\n",
        "    elif name == 'densenet201':\n",
        "        backbone = models.densenet201(pretrained=True).features\n",
        "    elif name == 'unet_encoder':\n",
        "        from unet_backbone import UnetEncoder\n",
        "        backbone = UnetEncoder(3)\n",
        "    else:\n",
        "        raise NotImplemented('{} backbone model is not implemented so far.'.format(name))\n",
        "\n",
        "    # specifying skip feature and output names\n",
        "    if name.startswith('resnet'):\n",
        "        feature_names = [None, 'relu', 'layer1', 'layer2', 'layer3']\n",
        "        backbone_output = 'layer4'\n",
        "    elif name == 'vgg16':\n",
        "        # TODO: consider using a 'bridge' for VGG models, there is just a MaxPool between last skip and backbone output\n",
        "        feature_names = ['5', '12', '22', '32', '42']\n",
        "        backbone_output = '43'\n",
        "    elif name == 'vgg19':\n",
        "        feature_names = ['5', '12', '25', '38', '51']\n",
        "        backbone_output = '52'\n",
        "    # elif name == 'inception_v3':\n",
        "    #     feature_names = [None, 'Mixed_5d', 'Mixed_6e']\n",
        "    #     backbone_output = 'Mixed_7c'\n",
        "    elif name.startswith('densenet'):\n",
        "        feature_names = [None, 'relu0', 'denseblock1', 'denseblock2', 'denseblock3']\n",
        "        backbone_output = 'denseblock4'\n",
        "    elif name == 'unet_encoder':\n",
        "        feature_names = ['module1', 'module2', 'module3', 'module4']\n",
        "        backbone_output = 'module5'\n",
        "    else:\n",
        "        raise NotImplemented('{} backbone model is not implemented so far.'.format(name))\n",
        "\n",
        "    return backbone, feature_names, backbone_output\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "\n",
        "    # TODO: separate parametric and non-parametric classes?\n",
        "    # TODO: skip connection concatenated OR added\n",
        "\n",
        "    def __init__(self, ch_in, ch_out=None, skip_in=0, use_bn=True, parametric=False):\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "\n",
        "        self.parametric = parametric\n",
        "        ch_out = ch_in/2 if ch_out is None else ch_out\n",
        "\n",
        "        # first convolution: either transposed conv, or conv following the skip connection\n",
        "        if parametric:\n",
        "            # versions: kernel=4 padding=1, kernel=2 padding=0\n",
        "            self.up = nn.ConvTranspose2d(in_channels=ch_in, out_channels=ch_out, kernel_size=(4, 4),\n",
        "                                         stride=2, padding=1, output_padding=0, bias=(not use_bn))\n",
        "            self.bn1 = nn.BatchNorm2d(ch_out) if use_bn else None\n",
        "        else:\n",
        "            self.up = None\n",
        "            ch_in = ch_in + skip_in\n",
        "            self.conv1 = nn.Conv2d(in_channels=ch_in, out_channels=ch_out, kernel_size=(3, 3),\n",
        "                                   stride=1, padding=1, bias=(not use_bn))\n",
        "            self.bn1 = nn.BatchNorm2d(ch_out) if use_bn else None\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # second convolution\n",
        "        conv2_in = ch_out if not parametric else ch_out + skip_in\n",
        "        self.conv2 = nn.Conv2d(in_channels=conv2_in, out_channels=ch_out, kernel_size=(3, 3),\n",
        "                               stride=1, padding=1, bias=(not use_bn))\n",
        "        self.bn2 = nn.BatchNorm2d(ch_out) if use_bn else None\n",
        "\n",
        "    def forward(self, x, skip_connection=None):\n",
        "\n",
        "        x = self.up(x) if self.parametric else F.interpolate(x, size=None, scale_factor=2, mode='bilinear',\n",
        "                                                             align_corners=None)\n",
        "        if self.parametric:\n",
        "            x = self.bn1(x) if self.bn1 is not None else x\n",
        "            x = self.relu(x)\n",
        "\n",
        "        if skip_connection is not None:\n",
        "            x = torch.cat([x, skip_connection], dim=1)\n",
        "\n",
        "        if not self.parametric:\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x) if self.bn1 is not None else x\n",
        "            x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x) if self.bn2 is not None else x\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Unet(nn.Module):\n",
        "\n",
        "    \"\"\" U-Net (https://arxiv.org/pdf/1505.04597.pdf) implementation with pre-trained torchvision backbones.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 backbone_name='resnet50',\n",
        "                 pretrained=True,\n",
        "                 encoder_freeze=False,\n",
        "                 classes=21,\n",
        "                 decoder_filters=(256, 128, 64, 32, 16),\n",
        "                 parametric_upsampling=True,\n",
        "                 shortcut_features='default',\n",
        "                 decoder_use_batchnorm=True):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.backbone_name = backbone_name\n",
        "\n",
        "        self.backbone, self.shortcut_features, self.bb_out_name = get_backbone(backbone_name, pretrained=pretrained)\n",
        "        shortcut_chs, bb_out_chs = self.infer_skip_channels()\n",
        "        if shortcut_features != 'default':\n",
        "            self.shortcut_features = shortcut_features\n",
        "\n",
        "        # build decoder part\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        decoder_filters = decoder_filters[:len(self.shortcut_features)]  # avoiding having more blocks than skip connections\n",
        "        decoder_filters_in = [bb_out_chs] + list(decoder_filters[:-1])\n",
        "        num_blocks = len(self.shortcut_features)\n",
        "        for i, [filters_in, filters_out] in enumerate(zip(decoder_filters_in, decoder_filters)):\n",
        "            print('upsample_blocks[{}] in: {}   out: {}'.format(i, filters_in, filters_out))\n",
        "            self.upsample_blocks.append(UpsampleBlock(filters_in, filters_out,\n",
        "                                                      skip_in=shortcut_chs[num_blocks-i-1],\n",
        "                                                      parametric=parametric_upsampling,\n",
        "                                                      use_bn=decoder_use_batchnorm))\n",
        "\n",
        "        self.final_conv = nn.Conv2d(decoder_filters[-1], classes, kernel_size=(1, 1))\n",
        "\n",
        "        if encoder_freeze:\n",
        "            self.freeze_encoder()\n",
        "\n",
        "        self.replaced_conv1 = False  # for accommodating  inputs with different number of channels later\n",
        "\n",
        "    def freeze_encoder(self):\n",
        "\n",
        "        \"\"\" Freezing encoder parameters, the newly initialized decoder parameters are remaining trainable. \"\"\"\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, *input):\n",
        "\n",
        "        \"\"\" Forward propagation in U-Net. \"\"\"\n",
        "\n",
        "        x, features = self.forward_backbone(*input)\n",
        "\n",
        "        for skip_name, upsample_block in zip(self.shortcut_features[::-1], self.upsample_blocks):\n",
        "            skip_features = features[skip_name]\n",
        "            x = upsample_block(x, skip_features)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "\n",
        "        \"\"\" Forward propagation in backbone encoder network.  \"\"\"\n",
        "\n",
        "        features = {None: None} if None in self.shortcut_features else dict()\n",
        "        for name, child in self.backbone.named_children():\n",
        "            x = child(x)\n",
        "            if name in self.shortcut_features:\n",
        "                features[name] = x\n",
        "            if name == self.bb_out_name:\n",
        "                break\n",
        "\n",
        "        return x, features\n",
        "\n",
        "    def infer_skip_channels(self):\n",
        "\n",
        "        \"\"\" Getting the number of channels at skip connections and at the output of the encoder. \"\"\"\n",
        "\n",
        "        x = torch.zeros(1, 3, 224, 224)\n",
        "        has_fullres_features = self.backbone_name.startswith('vgg') or self.backbone_name == 'unet_encoder'\n",
        "        channels = [] if has_fullres_features else [0]  # only VGG has features at full resolution\n",
        "\n",
        "        # forward run in backbone to count channels (dirty solution but works for any Module)\n",
        "        for name, child in self.backbone.named_children():\n",
        "            x = child(x)\n",
        "            if name in self.shortcut_features:\n",
        "                channels.append(x.shape[1])\n",
        "            if name == self.bb_out_name:\n",
        "                out_channels = x.shape[1]\n",
        "                break\n",
        "        return channels, out_channels\n",
        "\n",
        "    def get_pretrained_parameters(self):\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if not (self.replaced_conv1 and name == 'conv1.weight'):\n",
        "                yield param\n",
        "\n",
        "    def get_random_initialized_parameters(self):\n",
        "        pretrained_param_names = set()\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if not (self.replaced_conv1 and name == 'conv1.weight'):\n",
        "                pretrained_param_names.add('backbone.{}'.format(name))\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if name not in pretrained_param_names:\n",
        "                yield param"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwElZdQ_xbda"
      },
      "source": [
        "class UnetDownModule(nn.Module):\n",
        "\n",
        "    \"\"\" U-Net downsampling block. \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, downsample=True):\n",
        "        super(UnetDownModule, self).__init__()\n",
        "\n",
        "        # layers: optional downsampling, 2 x (conv + bn + relu)\n",
        "        self.maxpool = nn.MaxPool2d((2,2)) if downsample else None\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.maxpool is not None:\n",
        "            x = self.maxpool(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class UnetEncoder(nn.Module):\n",
        "\n",
        "    \"\"\" U-Net encoder. https://arxiv.org/pdf/1505.04597.pdf \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels):\n",
        "        super(UnetEncoder, self,).__init__()\n",
        "        self.module1 = UnetDownModule(num_channels, 64, downsample=False)\n",
        "        self.module2 = UnetDownModule(64, 128)\n",
        "        self.module3 = UnetDownModule(128, 256)\n",
        "        self.module4 = UnetDownModule(256, 512)\n",
        "        self.module5 = UnetDownModule(512, 1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.module1(x)\n",
        "        x = self.module2(x)\n",
        "        x = self.module3(x)\n",
        "        x = self.module4(x)\n",
        "        x = self.module5(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yaj_zfHk3frg"
      },
      "source": [
        "Unet().__module__\n",
        "Unet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhvF18PE80Uw"
      },
      "source": [
        "import inspect\n",
        "inspect.getmodule(Unet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t984m2IRq0gc"
      },
      "source": [
        "# Create Model\n",
        "model = Unet(backbone_name = Encoder, classes=1, encoder_freeze=True) # load pre-trained weights \n",
        "model_tfs =  Unet(backbone_name = Encoder, classes=1, encoder_freeze=False, pretrained=False)\n",
        "model.to(device)\n",
        "model_tfs.to(device);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_cpOfwyq43O"
      },
      "source": [
        "criterion = MixedLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "# Loop over all Epochs\n",
        "# CUDA_LAUNCH_BLOCKING=1\n",
        "epochs = 1\n",
        "train_loss_pretrained = []\n",
        "val_loss_pretrained = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss = train(train_loader,\n",
        "                       model,\n",
        "                       criterion,\n",
        "                       optimizer)\n",
        "    train_loss_pretrained.append(train_loss)\n",
        "\n",
        "    val_loss = evaluate(val_loader,\n",
        "                        model,\n",
        "                        criterion)\n",
        "    val_loss_pretrained.append(val_loss)\n",
        "\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    print(f'Training Loss: {train_loss}, \\t Validation Loss: {val_loss}\\n')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0duIpvzrHSj"
      },
      "source": [
        "# Loop over all Epochs\n",
        "# CUDA_LAUNCH_BLOCKING=1\n",
        "epochs = 1\n",
        "train_loss_tfs = []\n",
        "val_loss_tfs = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss = train(train_loader,\n",
        "                       model_tfs,\n",
        "                       criterion,\n",
        "                       optimizer)\n",
        "    train_loss_tfs.append(train_loss)\n",
        "\n",
        "    val_loss = evaluate(val_loader,\n",
        "                        model_tfs,\n",
        "                        criterion)\n",
        "    val_loss_tfs.append(val_loss)\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    print(f'Training Loss: {train_loss}, \\t Validation Loss: {val_loss}\\n')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPUtsdMRrMv0"
      },
      "source": [
        "fig = plt.figure(figsize=(15,7))\n",
        "fig.add_subplot(1,2,1)\n",
        "plt.plot(train_loss_pretrained, label='Imagenet Pretrained')\n",
        "plt.plot(train_loss_tfs, label='Train from sratch')\n",
        "plt.title('Train Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Dice Loss')\n",
        "plt.legend()\n",
        "\n",
        "fig.add_subplot(1,2,2)\n",
        "plt.plot(val_loss_pretrained, label='Imagenet Pretrained')\n",
        "plt.plot(val_loss_tfs, label='Train from sratch')\n",
        "plt.title('Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Dice Loss');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgalku_YSwG_"
      },
      "source": [
        "evaluate(val_loader,model,criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NriJmD4IEM4o"
      },
      "source": [
        "! pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9isTnjQE2yN"
      },
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rBEAlVRE-mA"
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVvEWVKDFEo-"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5vcb8u5g-a0"
      },
      "source": [
        "! kaggle competitions download siim-acr-pneumothorax-segmentation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6qwcI3kiHR_"
      },
      "source": [
        "! kaggle datasets download jesperdramsch/siim-acr-pneumothorax-segmentation-data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_36ifLjiRHh"
      },
      "source": [
        "! unzip siim-acr-pneumothorax-segmentation-data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ8bAUtsiydB"
      },
      "source": [
        "DATA_DIR = '/content/dicom-images-train'\n",
        "data_csv = '/content/pneumothorax/train-rle.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm5IjEgUts9C"
      },
      "source": [
        "! kaggle datasets download abhishek/siim-png-images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmTASUUJupyd"
      },
      "source": [
        "! unzip siim-png-images.zip "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}